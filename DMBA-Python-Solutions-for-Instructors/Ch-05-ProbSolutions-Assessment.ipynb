{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Evaluating Predictive Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> (c) 2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck \n",
    ">\n",
    "> _Data Mining for Business Analytics: Concepts, Techniques, and Applications in Python_ (First Edition) \n",
    "> Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n",
    ">\n",
    "> Date: 2020-03-08\n",
    ">\n",
    "> Python Version: 3.8.2\n",
    "> Jupyter Notebook Version: 5.6.1\n",
    ">\n",
    "> Packages:\n",
    ">   - dmba: 0.0.12\n",
    ">   - matplotlib: 3.2.0\n",
    ">   - pandas: 1.0.1\n",
    ">   - scikit-learn: 0.22.2\n",
    ">\n",
    "> The assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:19.805479Z",
     "iopub.status.busy": "2020-11-20T16:50:19.804283Z",
     "iopub.status.idle": "2020-11-20T16:50:20.909498Z",
     "shell.execute_reply": "2020-11-20T16:50:20.910195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "# import required packages for this chapter\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pylab as plt \n",
    "\n",
    "from dmba import regressionSummary, classificationSummary\n",
    "from dmba import liftChart, gainsChart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.915602Z",
     "iopub.status.busy": "2020-11-20T16:50:20.915049Z",
     "iopub.status.idle": "2020-11-20T16:50:20.918599Z",
     "shell.execute_reply": "2020-11-20T16:50:20.918135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Working directory:\n",
    "#\n",
    "# We assume that data are kept in the same directory as the notebook. If you keep your \n",
    "# data in a different folder, replace the argument of the `Path`\n",
    "# and then load data using \n",
    "#\n",
    "# pd.read_csv(‘filename.csv’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 \n",
    "\n",
    "A data mining routine has been applied to a transaction dataset and has classified 88 records as fraudulent (30 correctly so) and 952 as non-fraudulent (920 correctly so). Construct the confusion matrix and calculate the overall error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "classification confusion matrix\n",
    "|----------------------------------------------------------------------------|\n",
    "|             |                     Predicted Class                          |\n",
    "|----------------------------------------------------------------------------|\n",
    "| Actual Class|             C0               |             C1                |\n",
    "|----------------------------------------------------------------------------|\n",
    "|      C0     | n0,0 = number of correctly   | n0,1 = number of C0 cases     | \n",
    "|             |  classified C0 cases         |  incorrectly classified as C1 | \n",
    "|----------------------------------------------------------------------------|\n",
    "|      C1     | n1,0 = number of C1 cases in-| n1,1 = number of correctly    |\n",
    "|             |  coreectly classified as C0  |  classified C1 cases          |\n",
    "|----------------------------------------------------------------------------|\n",
    "\n",
    "Therefore in our problem the confusion matrix is\n",
    "\n",
    "classification confusion matrix\n",
    "|----------------------------------------------------------------------|\n",
    "|                    |                 Predicted Class                 |\n",
    "|----------------------------------------------------------------------|\n",
    "| Actual Class       |         Fraudulant(1)    |   Non-fraudulant (0) |\n",
    "|----------------------------------------------------------------------|\n",
    "| Fraudulant (1)     |             30           |        32            | \n",
    "|----------------------------------------------------------------------|\n",
    "| Non-fraudulant (0) |             58           |       920            |\n",
    "|----------------------------------------------------------------------|\n",
    "\n",
    "</pre>\n",
    "\n",
    "formula for overall error rate \n",
    "\n",
    "overall Error Rate = (n<sub>0,1</sub> + n<sub>1,0)</sub> / n, where n is the total number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.928099Z",
     "iopub.status.busy": "2020-11-20T16:50:20.927173Z",
     "iopub.status.idle": "2020-11-20T16:50:20.930153Z",
     "shell.execute_reply": "2020-11-20T16:50:20.930794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08653846153846154"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate = (32 + 58) / 1040\n",
    "error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the overall error rate is 8.65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2\n",
    "\n",
    "Suppose that this routine has an adjustable cutoff (threshold) mechanism by which you can alter the proportion of records classified as fraudulent. Describe how moving the cutoff up or down would affect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.2.a.__ the classification error rate for records that are truly fraudulent\n",
    "\n",
    "__5.2.b.__ the classification error rate for records that are truly nonfraudulent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "classification confusion matrix\n",
    "|--------------------------------------------------------|\n",
    "|                   |          Predicted Class           |\n",
    "|--------------------------------------------------------|\n",
    "| Actual Class      | Fraudulant (1) | Non-fraudulant (0)|\n",
    "|--------------------------------------------------------|\n",
    "| Fraudulant (1)    |       a        |         b         |\n",
    "|--------------------------------------------------------|\n",
    "| Non-fraudulant (0)|       c        |         d         |\n",
    "|--------------------------------------------------------|\n",
    "</pre>\n",
    "\n",
    "The classification error rate for truly fraudulent records (with this 0.5 cutoff) is b/(a+b)\n",
    "\n",
    "The classification error rate for truly non-fraudulent records (with this 0.5 cutoff) is c/(c+d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering the cutoff (here, below 0.5) leads to classifying more records, both fraudulent and non-fraudulent, as fraudulent: a and c both increase, b and d decline.\n",
    "\n",
    "__a.__ With respect to the classification error rate for truly fraudulent records, the error rate, b/(a+b), decreases as b goes up. As you lower the standard for calling a record fraudulent, you catch more and more of the real frauds.\n",
    "\n",
    "__b.__ With respect to the classification error rate for truly non-fraudulent records, the error rate, c/(c+d), increases as c goes up. As you lower the standard for calling a record fraudulent, you mistakenly identify more and more non-frauds as frauds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the cutoff (here, above 0.5) leads to classifying more records, both fraudulent and non-fraudulent, as non-fraudulent:  b and d both increase, a and c decline.\n",
    "\n",
    "__a.__ With respect to the classification error rate for truly fraudulent records, the error rate, b/(a+b), increases as b goes up. As you raise the standard for calling a record fraudulent, you miss more and more of the real frauds.\n",
    "\n",
    "__b.__ With respect to the classification error rate for truly non-fraudulent records, the error rate, c/(c+d), decreases as d goes up. As you raise the standard for calling a record fraudulent, fewer non-frauds get mis-labeled as frauds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3\n",
    "\n",
    "FiscalNote is a startup founded by a Washington, DC entrepreneur and funded by a Singapore sovereign wealth fund, the Winklevoss twins of Facebook fame, and others. It uses machine learning and data mining techniques to predict for its clients whether legislation in the US Congress and in US state legislatures will pass or not. The company reports 94% accuracy. (Washington Post, November 21, 2014, “Capital Business”)\n",
    "\n",
    "Considering just bills introduced in the US Congress, do a bit of internet research to learn about numbers of bills introduced and passage rates. Identify the possible types of misclassifications, and comment on the use of overall accuracy as a metric. Include a discussion of other possible metrics and the potential role of propensities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web research on govtrack.us shows that, in the 113th Congress (which covered 2013 and 2014), over 10,000 pieces of legislation were introduced but only 3% passed as enacted laws.  \"Enacted laws\" does not include the 6% that were passed as (usually meaningless) resolutions.  \n",
    "\n",
    "If we focus just on classifying each bill, and use overall accuracy as a metric, we could achieve 97% accuracy just by predicting that nothing will pass (as a law).   \n",
    "\n",
    "A data mining model might make two types of classification errors - saying that a bill will pass when it won't, and saying a bill won't pass when it will. The second type of error is probably more costly than the first - identifying the small number of bills that will pass is probably of more interest than overall accuracy. Therefore, a useful metric would be sensitivity to \"will pass\" - the proportion of \"will pass\" bills that were correctly predicted (alongside with specificity, which is the proportion of \"will not pass\" that are correctly ruled out).\n",
    "\n",
    "However, rather than just assigning a 0/1 class to each bill (classification), we will probably be more interested in ranking the bills and estimating a propensity (probability) for passage for each bill.  We would then focus on the high probability bills, and not be so concerned with the low probability bills.\n",
    "\n",
    "With ranking as our goal, we could use lift as a metric for how well a model separates out the \"will pass\" bills. As part of the calculation for lift, the bills would be ranked by their propensity to pass.  Lift gives a picture of how much better the model does than not using a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4\n",
    "\n",
    "Consider Figure 5.12, the decile lift chart for the transaction data model, applied to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.4.a.__ Interpret the meaning of the first and second bars from the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left-most bar: If we take the 10% \"most probable 1's (frauds)\" (as ranked by the model), it will catch 6.5 times as many 1's (frauds), as would a random selection of 10% of the records.\n",
    "\n",
    "#2nd bar from left: If we take the second highest decile (10%) of records that are ranked by the model as \"the most probable 1's (frauds)\" it will catch 2.7 times as many 1's (frauds), as would a random selection of 10% of the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.4.b.__ Explain how you might use this information in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a tax authority that wants to allocate their resources for investigating firms that are most likely to submit fraudulent tax returns. Suppose that there are resources for auditing only 10% of firms. Rather than taking a random sample, they can select the top 10% of firms that are predicted to be most likely to report fraudulently (according to the decile chart). Or, to preserve the principle that anyone might be audited, they can establish differential probabilities for being sampled -- those in the top deciles being much more likely to be audited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.4.c.__ Another analyst comments that you could improve the accuracy of the model by classifying everything as nonfraudulent. If you do that, what is the error rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following confusion matrix from Problem 5.1.\n",
    "\n",
    "<pre>\n",
    "classification confusion matrix\n",
    "|----------------------------------------------------------------------|\n",
    "|                    |                 Predicted Class                 |\n",
    "|----------------------------------------------------------------------|\n",
    "| Actual Class       |         Fraudulant(1)    |   Non-fraudulant (0) |\n",
    "|----------------------------------------------------------------------|\n",
    "| Fraudulant (1)     |             30           |        32            | \n",
    "|----------------------------------------------------------------------|\n",
    "| Non-fraudulant (0) |             58           |       920            |\n",
    "|----------------------------------------------------------------------|\n",
    "\n",
    "</pre>\n",
    "\n",
    "According to the new analyst our classification confusion matrix becomes-\n",
    "\n",
    "<pre>\n",
    "classification confusion matrix\n",
    "|----------------------------------------------------------------------|\n",
    "|                    |                 Predicted Class                 |\n",
    "|----------------------------------------------------------------------|\n",
    "| Actual Class       |         Fraudulant(1)    |   Non-fraudulant (0) |\n",
    "|----------------------------------------------------------------------|\n",
    "| Fraudulant (1)     |              0           |        88            | \n",
    "|----------------------------------------------------------------------|\n",
    "| Non-fraudulant (0) |              0           |       952            |\n",
    "|----------------------------------------------------------------------|\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.938273Z",
     "iopub.status.busy": "2020-11-20T16:50:20.937217Z",
     "iopub.status.idle": "2020-11-20T16:50:20.940721Z",
     "shell.execute_reply": "2020-11-20T16:50:20.941444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08461538461538462"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall misclassification rate\n",
    "\n",
    "error_rate = 88/1040\n",
    "error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the misclassification error rate is lower (8.46%) with the “everything non-fraudulent” proposal (although only slightly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.4.d.__ Comment on the usefulness, in this situation, of these two metrics of model performance (error rate and lift)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likely purpose of this analysis is to identify fraudulent records. The overall \"error rate\" is not likely to help much in evaluating competing methods for doing so. The key factor here is the ability to identify records that have a high probability of being fraudulent, and this is what lift measures. Using lift, you can \"descend\" through the records in order of probability of being fraudulent, knowing at each point how much more likely you are to be getting a fraudulent record than naively selecting at random. The \"error rate\" measure, by contrast, reveals nothing about the efficiency of identifying fraudulent records.\n",
    "\n",
    "The vast majority of records are non-fraudulent, and correctly classifying nonfraudulent records drives the overall error rate. One can achieve a very respectably low error rate just by classifying everything as non-fraudulent, which is not practically useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.5.__\n",
    "\n",
    "A large number of insurance records are to be examined to develop a model for predicting fraudulent claims. Of the claims in the historical database, 1% were judged to be fraudulent. A sample is taken to develop a model, and oversampling is used to provide a balanced sample in light of the very low response rate. When applied to this sample (n = 800), the model ends up correctly classifying 310 frauds, and 270 nonfrauds. It missed 90 frauds, and classified 130 records incorrectly as frauds when they were not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.5.a.__ Produce the confusion matrix for the sample as it stands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "classification confusion matrix\n",
    "|--------------------------------------------------------|\n",
    "|                   |          Predicted Class           |\n",
    "|--------------------------------------------------------|\n",
    "| Actual Class      |        1       |         0         |\n",
    "|--------------------------------------------------------|\n",
    "|          1        |      310       |        90         |\n",
    "|--------------------------------------------------------|\n",
    "|          0        |      130       |       270         |\n",
    "|--------------------------------------------------------|\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.946700Z",
     "iopub.status.busy": "2020-11-20T16:50:20.946145Z",
     "iopub.status.idle": "2020-11-20T16:50:20.950823Z",
     "shell.execute_reply": "2020-11-20T16:50:20.950332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Misclassification rate\n",
    "\n",
    "miscl_rate1 = (90 + 130) / 800\n",
    "miscl_rate1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the overall misclassification rate is 27.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model ends up classifying (310 + 130) / 800 = 0.55 = 55% of the records as fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.5.b.__ Find the adjusted misclassification rate (adjusting for the oversampling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add enough zeros so that the 1's only constitute 1% of the total and the 0's constitute 99% of the total (where is X is the total).\n",
    "<pre>                                                                                                           \n",
    "400 + 0.99*x = x\n",
    "Therefore x = 40, 000\n",
    "Number of zeros = 0.99 * 40, 000 = 39600\n",
    "\n",
    "#classification confusion matrix\n",
    "|-------------------------------------------------------------|\n",
    "|                   |          Predicted Class       |        |\n",
    "|-------------------------------------------------------------|\n",
    "|    Actual Class   |        1       |         0     | Total  |\n",
    "|-------------------------------------------------------------|\n",
    "|          1        |      310       |        90     | 400    |\n",
    "|----------------------------------------------------|--------|\n",
    "|          0        |    12870       |     26730     | 39600  |\n",
    "|----------------------------------------------------|--------|\n",
    "|       Total       |    13180       |     26820     | 40000  |\n",
    "|-------------------------------------------------------------|\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.955835Z",
     "iopub.status.busy": "2020-11-20T16:50:20.955242Z",
     "iopub.status.idle": "2020-11-20T16:50:20.959269Z",
     "shell.execute_reply": "2020-11-20T16:50:20.959689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.324"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall misclassification rate\n",
    "\n",
    "miscl_rate2 = (90 + 12870) / 40000\n",
    "miscl_rate2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model ends up classifying (310 + 12870) / 40000 = 0.3295 = 32.95% of the records as fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.5.c.__ What percentage of new records would you expect to be classified as fraudulent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above calculations, we expect 32.95% of the records to be classified as frauds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.6\n",
    "\n",
    "A firm that sells software services has been piloting a new product and has records of 500 customers who have either bought the services or decided not to. The target value is the estimated profit from each sale (excluding sales costs). The global mean is\n",
    "about 2128 dollars. However, the cost of the sales effort is not cheap— the company figures it comes to 2500 dollars for each of the 500 customers (whether they buy or not). The firm developed a predictive model in hopes of being able to identify the top spenders in the future. The cumulative gains and decile lift charts for the validation set are shown in Figure 5.13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.6.a__ If the company begins working with a new set of 1000 leads to sell the same services, similar to the 500 in the pilot study, without any use of predictive modeling to target sales efforts, what is the estimated profit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the 1000 new leads are like those in the pilot, then the company can expect the same mean profit per sale of 2500 dollars, or 2,500,000 dollars for the 1000 leads. This does not include the cost of the sales effort, which would cost an estimated 2.5 million dollars.  In other words, it would not be a profitable move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.6.b.__ If the firm wants the average profit on each sale to roughly double the sales effort cost, and applies an appropriate cutoff with this predictive model to a new set of 1000 leads, how far down the new list of 1000 should it proceed (how many\n",
    "deciles)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the profit must double the sales effort cost, that would be 5000 dollars. This is twice the average profit across all customers.  The company could achieve this by attempting sales to the first (top) decile among the customers, where the lift is about 2.1.  However, it should not go beyond this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.6.c__ Still considering the new list of 1000 leads, if the company applies this predictive model with a lower cutoff of 2500, how far should it proceed down the ranked leads, in terms of deciles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cutoff is lowered to 2500 dollars, a lift as low as 2500/2500 or 1.0 could be tolerated.  This would mean going all the way through the 5th decile, or half the customers. It would also mean that the product is breakeven at the margin (i.e. the last group of leads produces no net profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.6.d.__ Why use this two-stage process for predicting sales—why not simply develop a model for predicting profit for the 1000 new leads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model to predict overall profit for all 1000 leads would not be useful.  The profit would be zero.  Individual profit predictions for each of the 1000 leads would be useful, and might be sufficient, but generating staged solutions (corresponding to different cutoffs and differing lift) helps translate the optimization problem into a business problem, and frame a limited number of decision options for managers.  _Note:  different interpretations of the question are possible with respect to whether profit for all 1000 leads all together is intended, or profit for each of the 1000 leads individually, so some leeway is accorded in marking_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7\n",
    "\n",
    "Table 5.7 shows a small set of predictive model validation results for a classification model, with both actual values and propensities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.7.a.__ Calculate error rates, sensitivity, and specificity using cutoffs of 0.25, 0.5, and 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.969088Z",
     "iopub.status.busy": "2020-11-20T16:50:20.968294Z",
     "iopub.status.idle": "2020-11-20T16:50:20.971874Z",
     "shell.execute_reply": "2020-11-20T16:50:20.971382Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a data frame of data\n",
    "\n",
    "data = {'Propensity': [0.03, 0.52, 0.38, 0.82, 0.33, 0.42, 0.55, 0.59, 0.09, 0.21, 0.43, 0.04, 0.08, 0.13, 0.01, 0.79, 0.42, \n",
    "                       0.29, 0.08, 0.02],\n",
    "        'Actual': [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}\n",
    "\n",
    "# convert to data frame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.979072Z",
     "iopub.status.busy": "2020-11-20T16:50:20.978300Z",
     "iopub.status.idle": "2020-11-20T16:50:20.989205Z",
     "shell.execute_reply": "2020-11-20T16:50:20.989848Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.6000)\n",
      "\n",
      "       Prediction\n",
      "Actual 0 1\n",
      "     0 9 8\n",
      "     1 0 3\n"
     ]
    }
   ],
   "source": [
    "# cutoff = 0.25\n",
    "\n",
    "Predicted = [1 if p > 0.25 else 0 for p in df.Propensity]\n",
    "classificationSummary(df.Actual, Predicted, class_names=['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:20.995581Z",
     "iopub.status.busy": "2020-11-20T16:50:20.994933Z",
     "iopub.status.idle": "2020-11-20T16:50:20.999091Z",
     "shell.execute_reply": "2020-11-20T16:50:20.998086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Rate =  0.4\n",
      "\n",
      "Sensitivity= 1.0\n",
      "\n",
      "Specificity =  0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "# overall error rate\n",
    "error_rate = (8) / 20\n",
    "print('\\nError Rate = ', error_rate)\n",
    "# sensitivity\n",
    "sensitivity = (3) / (3+0)\n",
    "print('\\nSensitivity=',sensitivity)\n",
    "# specificity\n",
    "specificity = (9) / (8+9)\n",
    "print('\\nSpecificity = ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:21.005891Z",
     "iopub.status.busy": "2020-11-20T16:50:21.005227Z",
     "iopub.status.idle": "2020-11-20T16:50:21.009320Z",
     "shell.execute_reply": "2020-11-20T16:50:21.009744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9000)\n",
      "\n",
      "       Prediction\n",
      "Actual  0  1\n",
      "     0 15  2\n",
      "     1  0  3\n"
     ]
    }
   ],
   "source": [
    "# cutoff = 0.5\n",
    "\n",
    "Predicted = [1 if p > 0.5 else 0 for p in df.Propensity]\n",
    "classificationSummary(df.Actual, Predicted, class_names=['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:21.015497Z",
     "iopub.status.busy": "2020-11-20T16:50:21.014813Z",
     "iopub.status.idle": "2020-11-20T16:50:21.020768Z",
     "shell.execute_reply": "2020-11-20T16:50:21.019902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Rate =  0.1\n",
      "\n",
      "Sensitivity= 1.0\n",
      "\n",
      "Specificity =  0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "# overall error rate\n",
    "error_rate = (2) / 20\n",
    "print('\\nError Rate = ', error_rate)\n",
    "# sensitivity\n",
    "sensitivity = (3) / (3+0)\n",
    "print('\\nSensitivity=',sensitivity)\n",
    "# specificity\n",
    "specificity = (15) / (2+15)\n",
    "print('\\nSpecificity = ', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:21.031587Z",
     "iopub.status.busy": "2020-11-20T16:50:21.030506Z",
     "iopub.status.idle": "2020-11-20T16:50:21.037109Z",
     "shell.execute_reply": "2020-11-20T16:50:21.036319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9500)\n",
      "\n",
      "       Prediction\n",
      "Actual  0  1\n",
      "     0 17  0\n",
      "     1  1  2\n"
     ]
    }
   ],
   "source": [
    "# cutoff = 0.75\n",
    "\n",
    "Predicted = [1 if p > 0.75 else 0 for p in df.Propensity]\n",
    "classificationSummary(df.Actual, Predicted, class_names=['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:21.043401Z",
     "iopub.status.busy": "2020-11-20T16:50:21.042680Z",
     "iopub.status.idle": "2020-11-20T16:50:21.048394Z",
     "shell.execute_reply": "2020-11-20T16:50:21.047569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Rate =  0.05\n",
      "\n",
      "Sensitivity= 0.6666666666666666\n",
      "\n",
      "Specificity =  1.0\n"
     ]
    }
   ],
   "source": [
    "# overall error rate\n",
    "error_rate = (1) / 20\n",
    "print('\\nError Rate = ', error_rate)\n",
    "# sensitivity\n",
    "sensitivity = (2) / (2+1)\n",
    "print('\\nSensitivity=',sensitivity)\n",
    "# specificity\n",
    "specificity = (17) / (0+17)\n",
    "print('\\nSpecificity = ', specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.7.b.__ Create a decile lift chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-20T16:50:21.055547Z",
     "iopub.status.busy": "2020-11-20T16:50:21.053355Z",
     "iopub.status.idle": "2020-11-20T16:50:21.213333Z",
     "shell.execute_reply": "2020-11-20T16:50:21.212587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Decile Lift Chart'}, xlabel='Percentile', ylabel='Lift'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decile lift chart\n",
    "# sort data by propensities\n",
    "df = df.sort_values(by=['Propensity'], ascending=False)\n",
    "#create decile lift chart\n",
    "liftChart(df.Actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
