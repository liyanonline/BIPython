{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Overview of Data Mining Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> (c) 2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck \n",
    ">\n",
    "> _Data Mining for Business Analytics: Concepts, Techniques, and Applications in Python_ (First Edition) \n",
    "> Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n",
    ">\n",
    "> Date: 2020-03-08\n",
    ">\n",
    "> Python Version: 3.8.2\n",
    "> Jupyter Notebook Version: 5.6.1\n",
    ">\n",
    "> Packages:\n",
    ">   - pandas: 1.0.1\n",
    ">   - scikit-learn: 0.22.2\n",
    ">\n",
    "> The assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required functionality for this chapter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory:\n",
    "#\n",
    "# We assume that data are kept in the same directory as the notebook. If you keep your \n",
    "# data in a different folder, replace the argument of the `Path`\n",
    "DATA = Path('.').resolve().parent / 'data'\n",
    "FIGURES = Path('.').resolve().parent / 'figures' / 'chapter_02'\n",
    "FIGURES.mkdir(exist_ok=True, parents=True)\n",
    "# and then load data using \n",
    "#\n",
    "# pd.read_csv(DATA / ‘filename.csv’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.1 Supervised or Unsupervised Task\n",
    "\n",
    "Assuming that data mining techniques are to be used in the following cases, identify whether the task required is supervised or unsupervised learning.\n",
    "\n",
    "__2.1.a.__ Deciding whether to issue a loan to an applicant based on demographic and financial data (with reference to a database of similar data on prior customers).\n",
    "\n",
    "__Answer:__ This is supervised learning, because the database includes information on whether the loan was approved or not.\n",
    "\n",
    "__2.1.b.__ In an online bookstore, making recommendations to customers concerning additional items to buy based on the buying patterns in prior transactions.\n",
    "\n",
    "__Answer:__ This is unsupervised learning, because there is no apparent outcome (e.g., whether the recommendation was adopted or not).\n",
    "\n",
    "__2.1.c.__ Identifying a network data packet as dangerous (virus, hacker attack) based on comparison to other packets whose threat status is known.\n",
    "\n",
    "__Answer:__ This is supervised learning, because for the other packets the status is known.\n",
    "\n",
    "__2.1.d.__ Identifying segments of similar customers.\n",
    "\n",
    "__Answer:__ This is unsupervised learning because there is no known outcome (though once you use unsupervised learning to identify segments, you could use supervised learning to classify new customers into those segments).\n",
    "\n",
    "__2.1.e.__ Predicting whether a company will go bankrupt based on comparing its financial data to those of similar bankrupt and nonbankrupt firms.\n",
    "\n",
    "__Answer:__ This is supervised learning, because the status of the similar firms is known.\n",
    "\n",
    "__2.1.f.__ Estimating the repair time required for an aircraft based on a trouble ticket.\n",
    "\n",
    "__Answer:__ This is supervised learning, because there is likely to be knowledge of actual (historic) repair times of similar repairs.\n",
    "\n",
    "__2.1.g.__ Automated sorting of mail by zip code scanning.\n",
    "\n",
    "__Answer:__ This is supervised learning, as there is likely to be knowledge about whether the sorting was correct in previous mail sorting.\n",
    "\n",
    "__2.1.h.__ Printing of custom discount coupons at the conclusion of a grocery store checkout based on what you just bought and what others have bought previously.\n",
    "\n",
    "__Answer:__ This is unsupervised learning, if we assume that we do not know what will be purchased in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.2 \n",
    "\n",
    "Describe the difference in roles assumed by the validation partition and the test partition.\n",
    "\n",
    "__Answer:__ \n",
    "The validation partition is used to assess the performance of each supervised learning model so that we can compare models and pick the best one. In some algorithms (e.g., classification and regression trees, k-nearest neighbors) the validation partition may be used in automated fashion to tune and improve the model. This means that the validation data are actually used to help build the model. \n",
    "\n",
    "The test data partition is used for assessing the performance of the final chosen model on new data. The test data are not used to compare models, or to further tweak the model or improve its fit. (If the test data were used for these purposes, they would play a role in building or selecting the best model, and would no longer provide an unbiased assessment of the chosen model's performance with completely new data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.3 \n",
    "Consider the sample from a database of credit applicants in Table 2.16. Comment on the likelihood that it was sampled randomly, and whether it is likely to be a useful sample.\n",
    "\n",
    "![Table2.16.PNG](img/Table2.16.PNG)\n",
    "\n",
    "__Answer:__ This sample is not selected randomly as we can see from “observation #”, that there is pattern in the observations chosen for the sample. In particular, every 8th observation from the database was selected for the sample. When we select data with such a pre-decided methodology it might introduce a bias in the selected data set. This is true when the order of the observations in the dataset has some meaning (e.g., chronological order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.4 \n",
    "Consider the sample from a bank database shown in Table 2.17; it was selected randomly from a larger database to be the training set. _Personal Loan_ indicates whether a solicitation for a personal loan was accepted and is the response variable. A campaign is planned for a similar solicitation in the future and the bank is looking for a model that will identify likely responders. Examine the data carefully and indicate what your next step would be. \n",
    "\n",
    "\n",
    "![Table2.17.PNG](img/Table2.17.PNG)\n",
    "\n",
    "\n",
    "__Answer:__\n",
    "Since there are only 18 records and 9 predictor variables in the sample, the next step before building a model is to take a larger sample.  18 records is too few to support a model that considers 9 predictors.  How big a sample? The availability of data, the cost and effort involved in data handling, and software capabilities are the main constraining factors.  Also, it is useful to check the number of available responses, and the response ratio, in the larger database.  If the response ratio is very low, it would be worthwhile to oversample the cases where response is positive (or, in other words, undersample the non-response cases).  Two other issues:\n",
    "\n",
    "- Zip code should probably be aggregated at a higher level than 5 digits, which would likely produce an unmanageable number of predictor variables.  For example, it could be aggregated at the level of the first 3 digits.\n",
    "- It also seems that the key information in “Mortgage” is whether the person has a mortgage, and not so much the level of the mortgage, so some consideration could be given to converting this to a binary variable.\n",
    "\n",
    "See also the 2016 blog post by  Tom Fawcett: [Learning from Imbalanced Classes](http://www.kdnuggets.com/2016/08/learning-from-imbalanced-classes.html/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.5\n",
    "Using the concept of overfitting, explain why when a model is fit to training data, zero error with those data is not necessarily good.\n",
    "\n",
    "__Answer:__\n",
    "Overfitting occurs when the model captures not only the generalizable pattern in the data, but also the error. When we split the data into training and validation sets, we assume that the same pattern (if there is a pattern) exists in both, and that they differ only in the error that they contain. An absurd and false model may fit perfectly (on training data set) if the model has enough complexity. Therefore we may get zero error for such a model using the training dataset. Such a model, however, is not likely to give useful results on the validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.6 \n",
    "In fitting a model to classify prospects as purchasers or nonpurchasers, a certain company drew the training data from internal data that include demographic and purchase information. Future data to be classified will be lists purchased from other sources,\n",
    "with demographic (but not purchase) data included. It was found that “refund issued” was a useful predictor in the training data. Why is this not an appropriate variable to include in the model?\n",
    "\n",
    "__Answer:__\n",
    "The variable “refund issued” is unknown prior to the actual purchase, and therefore is not useful in a predictive model of future purchase behavior. In fact, “refund issued” can only be present for actual purchases but never for non-purchases. This explains why it was found to be closely related to purchase/non-purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.7\n",
    "A dataset has 1000 records and 50 variables with 5% of the values missing, spread randomly throughout the records and variables. An analyst decides to remove records with missing values. About how many records would you expect to be removed? \n",
    "\n",
    "__Answer:__ \n",
    "For a record to have all values present, it must avoid having a missing value (P = 0.95) for each of 50 records. The chance that a given record will escape having a missing value for two variables is 0.95 * 0.95 = 0.903. The chance that a given record would escape having a missing value for all 50 records is (0.95)^50 = 0.076945. This implies that 1-0.076944 = 0.9231 (92.31%) of all records will have missing values and would be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.8\n",
    "Normalize the data in Table 2.18, showing calculations.\n",
    "\n",
    "![Table2.18.PNG](img/Table2.18.PNG)\n",
    "\n",
    "__Answer:__\n",
    "Normalization of a measurement is obtained by subtracting the (column) average from each measurement and dividing the difference by the (column) standard deviation.\n",
    "\n",
    "For variable Age (years):\n",
    "Mean =44.66667 and Standard deviation (std) = 14.97554\n",
    "\n",
    "For variable Income ($):\n",
    "Mean=98.66667 and Standard deviation (std) = 62.86706\n",
    "\n",
    "For normalizing age for observation # 1 (Here age = 25):\n",
    "After subtracting the average and dividing by standard deviation, the normalized age = -1.438596.\n",
    "\n",
    "For normalizing income for observation # 1 (Here Income = 49000):\n",
    "After subtracting the average income and dividing by standard deviation, the normalized income = -0.865431.\n",
    "\n",
    "Let's normalize the data using sklearn's preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income\n",
      "0   25   49000\n",
      "1   56  156000\n",
      "2   65   99000\n",
      "3   32  192000\n",
      "4   41   39000\n",
      "5   49   57000\n"
     ]
    }
   ],
   "source": [
    "# import the required functionality for this problem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create a data frame\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, 56, 65, 32, 41, 49],\n",
    "    'Income': [49000, 156000, 99000, 192000, 39000, 57000]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age    Income\n",
      "0 -1.438597 -0.865431\n",
      "1  0.829022  0.999021\n",
      "2  1.487363  0.005808\n",
      "3 -0.926554  1.626314\n",
      "4 -0.268213 -1.039679\n",
      "5  0.316979 -0.726033\n"
     ]
    }
   ],
   "source": [
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "print(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.9\n",
    "Statistical distance between records can be measured in several ways. Consider Euclidean distance, measured as the square root of the sum of the squared differences. For the first two records in Table 2.17, it is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sqrt{(25-56)^2 + (49000-156000)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "Can normalizing the data change which two records are farthest from each other in terms of Euclidean distance?\n",
    "\n",
    "__Answer:__\n",
    "Yes, it can. By normalizing we equate the scales of the different variables, and therefore the Euclidean distance can dramatically change. In the example age is in single years while income is in thousands of dollars. The Euclidean distance on the raw data is therefore almost completely determined by income and unaffected by age. In contrast, after normalizing age and income will be on the same scale. Age will then have a much larger impact on the Euclidean distance. To see this in practice, examine the table below that compare the Euclidean distance before and after normalizing the data.\n",
    "\n",
    "We now see that records 4 and 5 (distance = 153000) are farthest from each other before normalizing, whereas records 1 and 3 are farthest from each other after normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>107000.004491</td>\n",
       "      <td>50000.016000</td>\n",
       "      <td>143000.000171</td>\n",
       "      <td>10000.012800</td>\n",
       "      <td>8000.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107000.004491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57000.000711</td>\n",
       "      <td>36000.008000</td>\n",
       "      <td>117000.000962</td>\n",
       "      <td>99000.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50000.016000</td>\n",
       "      <td>57000.000711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93000.005855</td>\n",
       "      <td>60000.004800</td>\n",
       "      <td>42000.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143000.000171</td>\n",
       "      <td>36000.008000</td>\n",
       "      <td>93000.005855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153000.000265</td>\n",
       "      <td>135000.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.012800</td>\n",
       "      <td>117000.000962</td>\n",
       "      <td>60000.004800</td>\n",
       "      <td>153000.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18000.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8000.036000</td>\n",
       "      <td>99000.000247</td>\n",
       "      <td>42000.003048</td>\n",
       "      <td>135000.001070</td>\n",
       "      <td>18000.001778</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1             2              3              4  \\\n",
       "0       0.000000  107000.004491  50000.016000  143000.000171   10000.012800   \n",
       "1  107000.004491       0.000000  57000.000711   36000.008000  117000.000962   \n",
       "2   50000.016000   57000.000711      0.000000   93000.005855   60000.004800   \n",
       "3  143000.000171   36000.008000  93000.005855       0.000000  153000.000265   \n",
       "4   10000.012800  117000.000962  60000.004800  153000.000265       0.000000   \n",
       "5    8000.036000   99000.000247  42000.003048  135000.001070   18000.001778   \n",
       "\n",
       "               5  \n",
       "0    8000.036000  \n",
       "1   99000.000247  \n",
       "2   42000.003048  \n",
       "3  135000.001070  \n",
       "4   18000.001778  \n",
       "5       0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euclidean distances for original data\n",
    "from sklearn.metrics import pairwise\n",
    "d = pairwise.pairwise_distances(df, metric='euclidean')\n",
    "pd.DataFrame(d, columns=df.index, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.935690</td>\n",
       "      <td>3.052916</td>\n",
       "      <td>2.543812</td>\n",
       "      <td>1.183284e+00</td>\n",
       "      <td>1.761101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.935690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191589</td>\n",
       "      <td>1.864280</td>\n",
       "      <td>2.315215e+00</td>\n",
       "      <td>1.799444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.052916</td>\n",
       "      <td>1.191589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.907409</td>\n",
       "      <td>2.043303e+00</td>\n",
       "      <td>1.380358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.543812</td>\n",
       "      <td>1.864280</td>\n",
       "      <td>2.907409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.746075e+00</td>\n",
       "      <td>2.660809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.183284</td>\n",
       "      <td>2.315215</td>\n",
       "      <td>2.043303</td>\n",
       "      <td>2.746075</td>\n",
       "      <td>2.107342e-08</td>\n",
       "      <td>0.663945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.761101</td>\n",
       "      <td>1.799444</td>\n",
       "      <td>1.380358</td>\n",
       "      <td>2.660809</td>\n",
       "      <td>6.639453e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3             4         5\n",
       "0  0.000000  2.935690  3.052916  2.543812  1.183284e+00  1.761101\n",
       "1  2.935690  0.000000  1.191589  1.864280  2.315215e+00  1.799444\n",
       "2  3.052916  1.191589  0.000000  2.907409  2.043303e+00  1.380358\n",
       "3  2.543812  1.864280  2.907409  0.000000  2.746075e+00  2.660809\n",
       "4  1.183284  2.315215  2.043303  2.746075  2.107342e-08  0.663945\n",
       "5  1.761101  1.799444  1.380358  2.660809  6.639453e-01  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euclidean distances for normalized data\n",
    "d_norm = pairwise.pairwise_distances(df_norm, metric='euclidean')\n",
    "pd.DataFrame(d_norm, columns=df_norm.index, index=df_norm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.10\n",
    "Two models are applied to a dataset that has been partitioned. Model A is considerably more accurate than model B on the training data, but slightly less accurate than model B on the validation data. Which model are you more likely to consider for final deployment?\n",
    "\n",
    "__Answer:__\n",
    "We prefer the model with the lowest error on the validation data. Model A might be overfitting the training data. We would therefore select model B for deployment on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.11\n",
    "The dataset *ToyotaCorolla.csv* contains data on used cars on sale during the late summer of 2004 in the Netherlands. It has 1436 records containing details on 38 attributes, including *Price, Age, Kilometers, HP,* and other specifications. \n",
    "\n",
    "We plan to analyze the data using various data mining techniques described in future chapters. Prepare the data for use as follows:\n",
    "\n",
    "__2.11.a.__ The dataset has two categorical attributes, Fuel Type and Color. Describe how you would convert these to binary variables. Confirm this using __pandas__ methods to transform categorical data into dummies. What would you do with the variable _Model_?\n",
    "\n",
    "__Answer:__\n",
    "The categorical fuel_type variable has three categories: petrol, diesel and CNG. To convert these variables into dummy variables, we use only two variables (here we use Petrol and Diesel, but a different pair can also be chosen). The binary variable Petrol gets the value 1 if Fuel Type=Petrol and otherwise it gets the value 0. The binary variable Diesel gets the value 1 if Fuel Type=Diesel and otherwise it gets the value 0. CNG is the \"reference category.\" If Fuel type is CNG, both binary variables take the value 0.\n",
    "\n",
    "Similarly, the variable _Color_ is converted to dummy variables. It resulted in 10 dummy variables with \"Color_Beige\" as the reference category.\n",
    "\n",
    "Making dummies of all the categories in _Model_ would make for a lot of predictor variables.  Some preliminary exploration should be done first, to see if there are a small number of models that account for most cases.  The analysis could be confined initially to those to see how important Model is as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(#Rows, #Columns): (1436, 39)\n",
      "\n",
      "\n",
      "First few records:\n",
      "\n",
      "   Id                                          Model  Price  Age_08_04  \\\n",
      "0   1  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13500         23   \n",
      "1   2  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13750         23   \n",
      "2   3  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13950         24   \n",
      "3   4  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  14950         26   \n",
      "4   5    TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors  13750         30   \n",
      "\n",
      "   Mfg_Month  Mfg_Year     KM Fuel_Type  HP  Met_Color  ... Powered_Windows  \\\n",
      "0         10      2002  46986    Diesel  90          1  ...               1   \n",
      "1         10      2002  72937    Diesel  90          1  ...               0   \n",
      "2          9      2002  41711    Diesel  90          1  ...               0   \n",
      "3          7      2002  48000    Diesel  90          0  ...               0   \n",
      "4          3      2002  38500    Diesel  90          0  ...               1   \n",
      "\n",
      "   Power_Steering  Radio  Mistlamps  Sport_Model  Backseat_Divider  \\\n",
      "0               1      0          0            0                 1   \n",
      "1               1      0          0            0                 1   \n",
      "2               1      0          0            0                 1   \n",
      "3               1      0          0            0                 1   \n",
      "4               1      0          1            0                 1   \n",
      "\n",
      "   Metallic_Rim  Radio_cassette  Parking_Assistant  Tow_Bar  \n",
      "0             0               0                  0        0  \n",
      "1             0               0                  0        0  \n",
      "2             0               0                  0        0  \n",
      "3             0               0                  0        0  \n",
      "4             0               0                  0        0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "\n",
      "Variables: Index(['Age_08_04', 'Mfg_Month', 'Mfg_Year', 'KM', 'HP', 'Met_Color',\n",
      "       'Automatic', 'CC', 'Doors', 'Cylinders', 'Gears', 'Quarterly_Tax',\n",
      "       'Weight', 'Mfr_Guarantee', 'BOVAG_Guarantee', 'Guarantee_Period', 'ABS',\n",
      "       'Airbag_1', 'Airbag_2', 'Airco', 'Automatic_airco', 'Boardcomputer',\n",
      "       'CD_Player', 'Central_Lock', 'Powered_Windows', 'Power_Steering',\n",
      "       'Radio', 'Mistlamps', 'Sport_Model', 'Backseat_Divider', 'Metallic_Rim',\n",
      "       'Radio_cassette', 'Parking_Assistant', 'Tow_Bar', 'Fuel_Type_Diesel',\n",
      "       'Fuel_Type_Petrol', 'Color_Black', 'Color_Blue', 'Color_Green',\n",
      "       'Color_Grey', 'Color_Red', 'Color_Silver', 'Color_Violet',\n",
      "       'Color_White', 'Color_Yellow'],\n",
      "      dtype='object')\n",
      "\n",
      "First few records in dummy variables:\n",
      "   Fuel_Type_Diesel  Fuel_Type_Petrol  Color_Black  Color_Blue  Color_Green  \\\n",
      "0                 1                 0            0           1            0   \n",
      "1                 1                 0            0           0            0   \n",
      "2                 1                 0            0           1            0   \n",
      "3                 1                 0            1           0            0   \n",
      "4                 1                 0            1           0            0   \n",
      "\n",
      "   Color_Grey  Color_Red  Color_Silver  Color_Violet  Color_White  \\\n",
      "0           0          0             0             0            0   \n",
      "1           0          0             1             0            0   \n",
      "2           0          0             0             0            0   \n",
      "3           0          0             0             0            0   \n",
      "4           0          0             0             0            0   \n",
      "\n",
      "   Color_Yellow  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "toyota_df = pd.read_csv(DATA / 'ToyotaCorolla.csv')\n",
    "# data dimension\n",
    "print(\"\\n(#Rows, #Columns):\", toyota_df.shape)\n",
    "print(\"\\n\")\n",
    "# review first few records\n",
    "print(\"First few records:\\n\")\n",
    "print(toyota_df.head())\n",
    "# create dummy variables for categorical variables, ignore the variable Model\n",
    "toyota_df = pd.get_dummies(toyota_df.iloc[:,3:39], prefix_sep='_', drop_first=True)\n",
    "# print column/variable names\n",
    "print(\"\\nVariables:\", toyota_df.columns)\n",
    "# review first few records in dummy variables\n",
    "print(\"\\nFirst few records in dummy variables:\")\n",
    "print(toyota_df.loc[:, 'Fuel_Type_Diesel':'Color_Yellow'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.11.b.__ Prepare the dataset (as factored into dummies) for data mining techniques of supervised learning by creating partitions in Python. Select all the variables and use default values for the random seed and partitioning percentages for training (50%),\n",
    "validation (30%), and test (20%) sets. Describe the roles that these partitions will play in modeling.\n",
    "\n",
    "__Answer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training :  (718, 45)\n",
      "Validation :  (430, 45)\n",
      "Test :  (288, 45)\n"
     ]
    }
   ],
   "source": [
    "# partition the data into training(50%), validation (30%) and test(20%) sets\n",
    "# random_state is set to a defined value to get the same partitions when re-running the code\n",
    "# training (50%), validation (30%), and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainData, temp = train_test_split(toyota_df, test_size=0.5, random_state=1)\n",
    "validData, testData = train_test_split(temp, test_size=0.4, random_state=1)\n",
    "print('Training : ', trainData.shape)\n",
    "print('Validation : ', validData.shape)\n",
    "print('Test : ', testData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training dataset__\n",
    "\n",
    "The training dataset is used to train or build models. For example, in a linear regression, the training dataset is used to fit the linear regression model, i.e. to compute the regression coefficients. This is usually the largest partition.\n",
    "\n",
    "__Validation dataset__\n",
    "\n",
    "Once a model is built on training data, we assess the accuracy of the model on unseen data. For this, the model should be used on a dataset that was not used in the training process. In the validation data we know the actual value of the response variable, and can therefore examine the difference between the actual value and the predicted value to determine the error in prediction. Based on this performance, sometimes the validation dataset is used to tweak the model, or to choose between multiple fitted models.\n",
    "\n",
    "__Test dataset__\n",
    "\n",
    "The validation dataset is often used to select a model with minimum error. Testing that model on completely unseen data gives a realistic estimate of the performance of the model. When a model is finally chosen, its accuracy with the validation dataset is still an optimistic estimate of how it would perform with unseen data. This is because (1) the final model has come out as the winner among the competing models based on the fact that its accuracy with the validation dataset is highest, and/or (2) the validation set was used to help build one or more models. Thus, you need to set aside yet another portion of data, which is used neither in training nor in validation, which is called the test dataset. The accuracy of the model on the test data gives a realistic estimate of the performance of the model on completely unseen data.\n",
    "\n",
    "__*It has been pointed out that there is a value in the cc variable - 16,000 - that is probably a data input error. The solutions have been prepared without correcting this error, but a solution that includes correcting this error to 1600 would also be fine. (The data could also be used as a small illustration or exercise of data prep and cleaning.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
